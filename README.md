# UG100

A dataset containing adversarial results for seven approximate attacks (+ MIP) on a subset of the MNIST and CIFAR10 test datasets. Specifically, it contains ~2.3k adversarial examples generated by the following attacks:
- Basic Iterative Method (`bim`)
- Brendel & Bethge Attack (`brendel`)
- Carlini & Wagner Attack (`carlini`)
- Deepfool (`deepfool`)
- Fast Gradient Sign Method (`fast_gradient`)
- Projected Gradient Descent (`pgd`)
- Uniform noise (`uniform`)
- MIPVerify (`mip`)

It also includes adversarial distances (for all attacks) and bounds (for MIP), as well as MIP convergence times.

Applications of this dataset include:

- Studying how, whenÂ and why adversarial attacks are close-to-optimal;
- Training classifiers that are robust to adversarial noise;
- Benchmarking new adversarial attacks.

The code used to generate UG100 can be found [here](https://github.com/samuelemarro/counter-attack).

# Installation

```
pip install ug100
```

# Implementation Notes

Since there aren't adversarial examples for every element of the test sets, we store the adversarials as an index-to-results dictionary.
For sequential access, use `IndexDataset`.

Additionally, we do not store the corresponding genuine examples for MNIST and CIFAR10. If you're using PyTorch, consider using [TorchVision's dataset library](https://pytorch.org/vision/stable/datasets.html).

# Citing this Dataset

Please cite this dataset as:
```
Samuele Marro and Michele Lombardi. _Asymmetries in Adversarial Settings_. 2022.
```